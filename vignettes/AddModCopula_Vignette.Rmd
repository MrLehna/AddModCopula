---
title: "Application of the AddModCopula Package"
date: "`r Sys.Date()`"
author: "Malte Lehna"
theme: journal
always_allow_html: yes
output: rmarkdown::html_document
bibliography: Sources.bib          
vignette: >
  %\VignetteIndexEntry{AddModCopula_Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}


---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
source("C:/Users/malte/Documents/Studium/Bachelor VWL/BA/AddModCopula/R/class_creg.R")
```
This is the manual to the AddModCopula package, in which we explain the functions of the package in more detail. We structured the manual as followed:

1. In the first section, the general theory with respect to additive copula models is shortly discussed. 
2. Thereafter, the structure of the input factors  is presented which also includes the choice of distributions. 
3. Based on the second section, we present the implementation of the global_creg() function and further analyze its output.  
4. In the fourth section, we apply the previous results of the global_creg() function in the creg_opt() function in order to to optimize the parameters of the additive model. 
5. Following the general explanations and examples of the aforementioned two functions, we address possible extensions in the last section.
```{r setup}
library(AddModCopula)
```
# 1. Theoretical structure of the AddModCopula package

The package was written to estimate the parameters of a bi-variate 
copula including the associated distributions. The method of choice is an
additive regression model which will be explained in more detail. 

The reason for the implementation of a bi-variate copula is to model a joint 
distribution between two continuous random variables  $X_1$ and $X_2$. However, this joint distribution can be rather complex  in practice . Therefore it is easier to construct the relationship between the two continous random variables through a copula and their individual marginal distributions. In general, a copula can be defined as a function that combines the marginal distributions of multiple random variables in order to describe their joint distribution. Based on the theorem of Sklar, we are able to describe this relationship in a two dimensional approach. Thus, according to Sklar we postulates that a joint distribution $F_{1,2}(X_1,X_2)$ of two continuous random variables can be described through a uniquely defined copula.
$$F_{1,2}(X_1,X_2) = C(F_{1}(X_{1} ), F_{2}(X_{2} ) )$$
In terms of the additive model setting, we now consider that both variables $X_1$ and $X_2$ have unique distributions which are influenced by external variables $\boldsymbol{\nu}$ e.g. prior informations. Thus, the Sklar theorem can be reformulated to the following equation.  
$$F_{1,2}(X_1,X_2 \mid \boldsymbol{\nu}) = C(F_{1}(X_{1} \mid \boldsymbol{\nu}), F_{2}(X_{2} \mid \boldsymbol{\nu}) \mid \boldsymbol{\nu})$$
Correspondingly, the copula describes a joint distribution through combining both marginal distributions $F_{1}(X_1 \mid \boldsymbol{\nu})$ and $F_{2}(X_2 \mid \boldsymbol{\nu})$. If both marginal distributions are correctly identified and prior information $\boldsymbol{\nu}$ is known as well, one can model the joint distribution adequately. 

Accordingly, the identification of the distributional parameters as well as the correlation parameters of the copula are necessary in order to describe the joint distribution. Within the AddModCopula package, these parameters are estimated through an additive regression model based on the paper of [@Klein2016] as well as the Master thesis of [@Johann].  In order to describe the additive model structure, we first have to define the underlying distributional parameters. In the case of the continous random variables $X_1$ and $X_2$ we define their distribution quite general:
$$ 
X_1 \sim f(\boldsymbol{\theta}^{(1)})=f(\theta_1^{(1)},\theta_2^{(1)},...)\\
X_2 \sim g(\boldsymbol{\theta}^{(2)})\\
$$
Both $f()$ and $g()$ are probability density functions (pdf) which are dependent on one or more distributional parameters ($\boldsymbol{\theta}^{(1)}$ and $\boldsymbol{\theta}^{(2)}$) that describe the respective probability mass. Note that through the general definition we allow for a variety of distributions in our package. 


With this general parameter definition, we now formulate the additive regression model. Here the primary goal of the model is to estimate the distributional parameters in such manner that they explain the observations of  continous random variables $X_1$ and $X_2$ in the most probable way. For the estimation, we
implement the prior information $\boldsymbol{\nu}$ as external variables in the model, while the distributional parameters $\boldsymbol{\theta}^{(1)}$ and $\boldsymbol{\theta}^{(2)}$ are seen as endogenous variables. Additionally, we include the copula parameter as an estimable parameter as well and we denote it by $\boldsymbol{\theta}^{(3)}$. 
However, due to the fact that in many cases the parameters of the distributions are restricted to a specific parameter space, we introduce a transformation function prior to the additive model. Thus, for each model parameter $\theta_i^{(j)}$ a transformation function $h_{ij}()$ is implemented:
$$\theta_i^{j}= h_{ij}(\eta_i^{j}) \\
\eta_i^{j} =  h_{ij}^{\ \ -1} (\theta_i^{j})
$$
Note that these transformation functions can differ across  the parameters and distributions. In addition, the transformation can further be applied to enhance the optimization process. Given the transformed $\eta_i^{j}$ values, we are now able to construct the additive model. For this purpose, we store the prior information $\boldsymbol{\nu}$ for each $\eta_i^{j}$ in respective $Z_{ij}$ matrices. Thereafter, we formulate the following additive regression equation:
$$
 \eta_i^{j} = \beta_{0ij} +\beta_{1ij}*z_{1ij} + ... +\beta_{lij}*z_{lij}\\
 =\mathbf{Z}_{ij}\boldsymbol{\beta_{ij}}
$$
Note that $j=1,2,3$ depicts the respective distribution/copula, while $i=1,..5$ denotes the number of parameters within each distribution. The number of external parameters $l$ is in this case arbitrary, with $l=0$ defining an intercept model. Moreover, it is also possible to formulate a global additive model, by combining all parameters in an $\boldsymbol{\eta}$ matrix. Note that the parameter within the matrix $\boldsymbol{\eta}$ are not only varying across distributions but are also variable over the observations. The $\boldsymbol{\eta}$ matrix is therefore defined as followed: 
$$ \boldsymbol{\eta} = \beta_{0}\mathbf{1}_{n} +\mathbf{Z}_{1}\boldsymbol{\beta}_{1} + ... + \mathbf{Z}_{L}\boldsymbol{\beta}_{L} $$
In order to describe and optimize these regression models, we define a likelihood based on the joint density $p_{1,2}$ of the joint distribution $F_{1,2}(X_1,X_2)$. For this purpose, we define the marginal densities of $F_{1}(X_{1})$ and $F_{2}(X_{2})$ as $p_1(\theta^{(1)}\mid \boldsymbol{\nu})$ and $p_2(\theta^{(2)}\mid \boldsymbol{\nu})$. Furthermore, recall that also the copula is dependent on the external information. Thus the copula density can be described by $c(\boldsymbol{\theta}^{(3)}\mid \boldsymbol{\nu})$.
Based on the marginal densities, we are now able to describe the joint density for the observation $x_{k1}$ and $x_{k2}$ (with $k=1,...,n$) of the continous random variabes as followed:  
$$p_{1,2}(x_{k1},x_{k2}\mid \boldsymbol{\nu}) = p_1(\boldsymbol{\theta_k}^{(1)}\mid \boldsymbol{\nu})(x_{k1}) \ \  p_2(\boldsymbol{\theta_k}^{(2)}\mid \boldsymbol{\nu})(x_{k2}) \ \  c(\boldsymbol{\theta_k}^{(3)}\mid \boldsymbol{\nu})  $$ 
As one can see, the joint density of the two variables is described, by combining the two marginal distributions with the density of the copula function.
However, recall that due to the different values within the Z matrices, it is possible that the parameters $\eta_i^{(j)}$ and therefore $\theta_i^{(j)}$ change across observation. Therefore, each $\boldsymbol{\theta}^{(j)}$ vector has an index k in the previous equation.

In order to describe the overall likelihood of the parameters, we now only have to combine the joint densities of the individual observations. Thus, the total log-likelihood of all observations can be formulated as the sum of all logarithmic joint densities: 
$$
l=\sum_{k=1}^{n}\log \left(p_{1,2}(x_{k1},x_{k2}\mid \boldsymbol{\nu}) \right) =\sum_{k=1}^{n}\log \left(p_1(\boldsymbol{\theta_k}^{(1)}\mid \boldsymbol{\nu})(x_{k1}) \ \  p_2(\boldsymbol{\theta_k}^{(2)}\mid \boldsymbol{\nu})(x_{k2})\ \ c(\boldsymbol{\theta_k}^{(3)}\mid \boldsymbol{\nu}) \right)
$$
Based on this total log-likelihood, we are now able to estimate the respective parameters $\boldsymbol{\theta}^{(j)}$ given the data set of $X_1$,$X_2$ as well as the external variables $\boldsymbol{\nu}$. Within the additive model, the variable of interest are the beta coefficients of the Z matrices which describe the effect of the exogenous variable. 

After this short introduction of the theoretical composition of the additive regression copula model, we will now analyze the structure of the functions within this package. 

# 2. Discussion of the input format of the data  
Before reviewing the implementation of the previous discussed model in the global_creg() function, we first have to talk about the required data. Therefore, we begin by analyzing the observations of $X_1$ and $X_2$. Afterwards, the structure of the exogenous variables (i.e. Z-Matrices and betas) are discussed which are vital for the functionality of the function. As a final part, the input options for the distributions and copulas are presented. To give a better understanding, we will use the example data set within this vignette to model the underlying structure.

To begin with, the first input variable for the global_creg () function are the observations of the continuous variables. 
Due to the fact, that this package only supports a bi-variate 
copula with two distributions, the underlying data needs to be in a two 
dimensional form as well. This statement also holds for the example data set, 
were we provide the wind data of the German mountain 'Zugspitze'. However, 
for legal reasons this data is a synthetic dataset which was created from the 
original data. Nevertheless, the first dimension is the wind speed and the second dimension is 
the wind direction. Note that because the previous observations are used as 
explanatory data, the first ten observations are discarded as lags. 
```{r}
data("zugspitze_synthetic_dataset")
obs <- length(zugspitze_synthetic_dataset$ws)
dat <- cbind(zugspitze_synthetic_dataset$ws[10:obs], zugspitze_synthetic_dataset$wd[10:obs])
head(dat)
```

The second input factors are the components of the additive model i.e. $Z_{ijl} \boldsymbol{\beta}_{ijl}$ . As seen in 
the theoretical explanation, each parameter of the distributions and the 
copula, are described by the additive model. Consequently, the model is 
constructed with a Z matrix and a beta vector for each parameter. Due to the fact that the global_creg () function was constructed quite liberal, in terms of the number of parameters per distribution, the input structure of the Z-Matrix is essential. Accordingly up to 5 parameters per distribution are allowed within the global_creg function.  Based on the format of the Z input the function determines not only the number of parameters per distribution($i$) but also the numbers of betas for each parameters ($l$). Therefore, we will discuss the 
Z-matrix input in more detail. 

In order to display the structure and the numbers of parameters correctly, the 
function requires a list object as Z-matrix input which in turn consists of 
three list object. 
The first and second object are the parameters of the corresponding 
distribution, while the third object is the parameter matrix of the copula. 
As input for each list object there are two possible options. If only one
parameter is required, a matrix or even vector can be implemented. If more then 
one parameter is necessary for the distribution/copula to work, a additional 
list needs to be implemented. 

In case of the example, a Weibull distribution for the wind speed and a von
Mises distribution for the wind direction were chosen to model the underlying 
data.  For both distributions, two parameters are required in order 
to describe their marginal density. Moreover, as copula we chose the Frank copula which requires another parameter, thus resulting in a total of five 
parameters for each observation. Note again that as explanatory variables 
of the distributional parameters, the model is constructed with previous
observations of the wind data. 
```{r}
Zlist <- list(
  # Weibull
  p1=list(
    scale = cbind(rep(1, obs-9), abs(zugspitze_synthetic_dataset$ws[9:(obs-1)])),
    shape_a = cbind(rep(1, obs-9), abs(zugspitze_synthetic_dataset$ws[9:(obs-1)]))),
  # von Mises dist
  p2=list(
    mu = cbind(rep(1, obs-9), zugspitze_synthetic_dataset$dwd[9:(obs-1)]),
    kappa = cbind(rep(1, obs-9), zugspitze_synthetic_dataset$dwd[9:(obs-1)])),
  # Copula
  theta = cbind(rep(1, obs-9))
)
```
As one can see, the parameters of both distributions are incorporated through 
another list within the list, with two matrices for each parameter. Note that for obvious 
reasons, all Z matrices and the data matrix have to have the same length of 
observations (i.e. the same row length).

The second part of the additive models are the beta coefficients which define
the effect of the explanatory variables. Even though these are the variables of interest, the global_creg() function requires some starting values. However in contrast to the Z-Matrix, the beta values one require a vector input with the correct length. In case of the example, the 
betas are set to one in order to offer adequate starting values:
```{r}
startbeta <- rep(1,9)
```
Note that the optimization of these coefficients is conducted within the the second function of the AddModCopula package. This function is further 
explained in the fourth section of the vignette. After the format of the input data was presented, we will now address the structure of the distribution and the copula. 

Beginning with the distributions, there are two possible ways to implement the 
distributions in the global_reg() function. The first (and preferred) way is the implementation through a list object. In order to calculate the likelihood of the data, it is necessary that the list includes both the density (dx) as well as the distribution function (px) i.e. cdf. 
Moreover, the list elements also have to be in that order, as seen in the example code: 
```{r}
dist1 <- list(dweibull,pweibull)
```
However, some things have to be noted. First of all, it is essential that within
the list both elements are written without a bracket i.e. list(dweibull(),
pweibull()) will return an error statement. Moreover, if an individual
distributions needs to be implemented, the format should be similar to the generic
distributions of the stats package as defined in [@stats]. See second distribution or last chapter for more information. 

In addition to the list implementation, it is also possible to include distributions from the distr package from [@distr]. Due to there specific format, their realization is as followed: 
```{r message=FALSE, warning=FALSE}
library(distr)
```
```{r}
dist1 <- Weibull
```
Note again that in order to work, the distribution needs to be implemented 
without a bracket. The reason, why the implementation through the 
list is preferable is that the distributions of the dist have some 
performance disadvantages which is especially critically for the later optimization. 

The second distribution in question is the von Mises distribution which can 
either be implemented with the circular package (see [@circular]) or one can use the 
pre-programmed function. Note that the second version to implement the von Mises distribution was specifically build for internal usage. 
```{r message=FALSE, warning=FALSE}
library(circular)
```
```{r}
# Version 1: 
# Circular package
 pfunc <- function(q,par1,par2){
   return(pvonmises(q=circular(q),mu=circular(par1),kappa = par2,
                    from=circular(0)))
 }
 dfunc <-function(x,par1,par2){
   return(dvonmises(x=circular(x),mu=circular(par1),kappa = par2,log=FALSE))
 }
dist2 <- list(dfunc, pfunc)

# or

# Version 2: 
# Implemented version
# dist2 <-"vonMises"
 
```

Next to the implementation of the distributions, there are also different 
choice options for the copula. Within the package, three archimedian 
copulas are implemented in order to describe the two dimensional data. 
These copulas are the "Frank", "Gumbel" and "Clayton" copula. In addition, we
also implemented the "Gaussian" copula as well. For further description of 
the copula we refer to the paper of LINK!.# Note that it is also possible to 
implement other copulas which will be discussed in the last chapter. 
In case of the example, the Frank copula was chosen: 
```{r}
copula <- "Frank"
```


# 3. Implementation of the global_creg() function 
After the overall discussion of the input structure, we can now implement the 
global_creg() function. However prior to the calculation, it is possible to define a transformation
function $h()$ in order to transform the parameters, i.e. recall $\theta_i^{j}= h_i^{j}(\eta_i^{j})$

Within the global_creg() function, the transformation is conducted after the calculation of the additive model, thus assuring that the parameters are in the correct parameter space.
A simple example is the Poisson distribution, where one has to ensure that the lambda
parameters are positive integers, thus some adjustments are necessary.
Next to the necessary constrains, the transformation can also have a second function if wanted. 
Through a smart transformation (e.g. scaling) it is possible to reduce the computational times of the 
optimization process. Note that it is possible to view the parameters prior and after the transformation with the summary 
function, as described below. 

In case of the example, we want to build a complex transformation function, as seen below: 
```{r}
# Again, using the implemented von Mises distr. in order to transfrom the 
# mu parameter in the param_trans function. 


transfct <- function(param){
  param <- exp(param/100)
  param[,3] <- (param[,3]/(param[,3] + 1)) * 2 * pi
  return(param)
}
```
Within the code, the parameters are scaled exponentially in order to have a positive scale and in addition are divided through 100 to ensure smaller parameters. Moreover, the mu parameter re-transformed onto a scale between zero and $2\pi$. Keep in mind that this transformation function has to be conducted with caution, because the transformation of specific columns has to correspond with the correct parameter. 

Nevertheless, after all input variables are defined, it is now possible to 
implement the global_creg() function: 
```{r}
result <- global_creg(beta = startbeta,Z = Zlist,
                      data = dat,param_trans=transfct,
                      dist1 = dist1,dist2 = "vonMises",copula = "Frank")

```
The output of the global_creg() function is a large list which includes 
both the input variables as well as the results of the calculation. Thus it is possible to access all results in the list object.  Moreover, the output is defined as a specific S3 class with generic functions. Per default, the printing function the global_creg() function returns the total 
log-likelihood for the given beta and Z matrices. 
```{r}
print(result)
```

Next to the print function, other generic functions are implemented which are  the summary function, the plot function and the AIC/BIC functions. In case of the summary function, it is possible to receive the parameters of the distributions/copula by setting param=TRUE. This step was incorporated in order to double check the results of the transformation function. 
```{r}
summary(result)
out <- summary(result,param = TRUE)
head(out$`Parameter before transformation`)
head(out$`Parameter after transformation`)
```
The next generic function is the plot() function which uses ggplots to 
visualize the results. Similar to other generic plot functions, it is possible 
to receive different plots, by including the additional parameter option e.g. plot(x,option="likelihood3D"). The following plots are available within the 
option argument. "likelihood2D" and "likelihood3D" display the individual likelihood 
per observation in a 2D or 3D plot.  In addition, it is also possible
to plot the results of the copula density by setting option="copula3D". If 
option="dataview" is selected, three plots for the data set are plotted. The 
default selection for option is "likelihood2D".
```{r}
 plot(result)
 plot(result,option="likelihood3D")
 plot(result,option="dataview")
# ?plot.creg to list all options
```
Finally, the last two generic functions are the AIC and the BIC. 
They are computed with the total log-likelihood. 
```{r}
AIC(result)
BIC(result)
```


# 4. Optimization with the creg_opt()
After the explanation of the global_creg() function, we will now use the results 
to optimize for the beta coefficients with the creg_opt() function. In general, 
the creg_opt() function relies on the basic optim() function, however additional
options are available. 

To begin with, we have to insert the previous data into the creg_opt function, 
which can be done by two ways. Either, a creg object form a previous global_creg() calculation can be implemented or all parameters are inserted manually.
```{r eval=FALSE}
#Do not run, due to computation time. (Maxit still only 10)
# Option 1
creg_opt(result,
         method = "L-BFGS-B",
         cores = 1,
         maxit = 10,
         infinity_control = NULL)

# 2. Option 
creg_opt(Z = Zlist,  data =dat, dist1 = dist1,dist2 ="vonMises",
         copula = "Frank",startbeta = NULL, param_trans = transfct,
         method = "L-BFGS-B",
         cores = 1,
         maxit = 10,
         infinity_control = NULL,
         recalculate=FALSE
)
```
Depending on the input choice, there are different ways to assign starting beta values. In the first option, the creg_opt() function takes the beta values which were implemented within the global_creg function. Otherwise, one can assign a specific value through the startbeta variable. If no beta values are supplied (as seen in Option 2.) then function will assign a 
default beta value of 1 to all coefficients. 

Furthermore, there are some options which are similar to the basic optim() function.
First of all, it is possible to assign different maximization methods, 
which correspond to the basic function. Secondly, the maxit can also be defined, 
which accounts for the number of iterations within the optimization process. 

However, there are also two new options available. The first one is the cores 
option. If available, users can select multiple cores in order to decrease the 
calculation time. In the background, the creg_opt() function then uses the 
optimParallel package in order to estimate the coefficients, see [@optimP]. However, there are 
some restrictions, because the optimParallel function only accepts the 
"L-BFGS-B" as an estimation method. Furthermore, errors might occur on a 
Windows environment, because the R version on Windows does not support parallel
computing. Nevertheless, if possible we advise to run the estimation with multiple 
cores, in order to reduce calculation time. 

The second option is the infinity control. Within the estimation process, it is 
possible that the global_creg() functions might return infinity values. In many
cases, the optimization methods are able to handle this problem, except for the 
"L-BFGS-B" method. Accordingly for the "L-BFGS-B" method, the infinity values 
(or negative infinity values) of the log-likelihood are replaced by 10e50 
(or -10e50 respectively). If a different value is desired, the users can insert 
their individual lower/upper bound for the total log-likelihood into the 
infinity_control variable. This switch is conducted for all methods. 

Moreover, as a small third choice it is possible to recalculate the model within
the creg_opt() function. By setting recalculate==TRUE, the optimal parameters are
implemented into the function and the return is similar to the global_creg() 
function.


Overall, the output of the creg_opt() function is similar to the normal optim() function.
Therefore the function calls are similar (see ?optim for more information).

If you interested in optimizing the global_creg() function yourself, keep in mind that it returns a list object. Thus, it is necessary to extract the log-likelihood component by the following statement: 
```{r}
loglikelihood <- result$result$Likelihood
```

# 5. Further extensions
In the last section, we shortly address the individual distributions and copulas which can be supplied for the global_creg() function. It is important to note that for the copula, you have to supply the density of the copula and not the general structure 

In terms of the self written distributions, we already discussed the necessary shape with the von Mises distribution. Recall, that the correct order within the list argument has to be the density d() and then the cdf with p():
```{r}
 pfunc <- function(q,par1,par2){
   return(pvonmises(q=circular(q),mu=circular(par1),kappa = par2))
 }
 dfunc <-function(x,par1,par2){
   return(dvonmises(x=circular(x),mu=circular(par1),kappa = par2))
 }
dist2 <- list(dfunc, pfunc)
```
A few things have to be remarked. First of all, the distributions are only 
allowed to have one input value for the q/x. (Within the global_creg function both the density and the cumulative density of the observation (q,x) are calculated.) Moreover, distributions with up to 5 parameters are supported. However, each parameter has to be implemented as a 
scalar. See the following examples for a WRONG implementation:  
```{r eval=FALSE}
# The following things will not work!:
wrongfunct(q1,q2,par1,par2,...)
wrongfunct(q,par1=c(1,2,3),par2=c(1,2,3),...)
```

For an individual copula density, a function needs to be implemented in the copula 
element. Here, the structure for the function needs to be the following:  
```{r eval=FALSE}
copula <- function(data1,data2,theta,ddist1,pdist1,ddist2,pdist2){
  x <- runif(n = length(data1),0,1) # Example calculation for within the copula
 return(x)  
}
```
As one can see, the structure of the input is critical for the functionality of 
the global_creg function, with the following restrictions: 

1. All input except theta (thus data1,data2,ddist1,pdist1,ddist2,pdist2) are in a 
vector format with the same length.
2. The theta can both be a vector or a matrix if more parameters are required. 
3. Logically, the data1 corresponds to the ddist1 and pdist1, same holds for 
data2, ddist2,pdist2.
4. ddist1 describe the result by applying the dist() function onto the corresponding distribution with the parameters. 
5. The output needs to be a vector of the same size and needs to be between 
0 and 1. 

# References:
